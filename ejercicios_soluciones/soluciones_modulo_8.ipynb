{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Módulo 8: Temas avanzados.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 8.2.  Web Scraping I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.2.1** Captura los hechos relevantes de la CNMV: http://www.cnmv.es/portal/HR/HRAldia.aspx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Firefox'}\n",
    "url = 'http://www.cnmv.es/portal/HR/HRAldia.aspx'\n",
    "re = requests.get(url,headers=headers)\n",
    "soup = BeautifulSoup( re.text, 'html.parser')\n",
    "datos_a_extraer = soup.find_all('article',{'class' : 'block-content-pdf block-content'})\n",
    "datos_a_extraer = datos_a_extraer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_hora = []\n",
    "horas = datos_a_extraer.find_all('li',{'class' : 'time'})\n",
    "for hora in horas:    \n",
    "    texto = hora.text\n",
    "    texto = texto.replace('\\r\\n',\"\")\n",
    "    #print(texto)\n",
    "    col_hora.append(texto)\n",
    "\n",
    "col_titulos = []\n",
    "titulos = datos_a_extraer.find_all('span',{'class' : 'tit-small'})\n",
    "for titulo in titulos:    \n",
    "    texto = titulo.text\n",
    "    #print(texto)\n",
    "    col_titulos.append(texto)\n",
    "\n",
    "col_tipo = []\n",
    "tipologias = datos_a_extraer.find_all('span',{'class' : 'negrita'})\n",
    "for tipo in tipologias:    \n",
    "    texto = tipo.text\n",
    "    #print(texto)\n",
    "    col_tipo.append(texto)\n",
    "\n",
    "col_contenido = []\n",
    "contenidos = datos_a_extraer.find_all('li',{'class' : 'padding-r resumen'})\n",
    "for contenido in contenidos:    \n",
    "    texto = contenido.text\n",
    "    texto = texto.replace('\\r\\n',\"\")\n",
    "    #print(texto)\n",
    "    col_contenido.append(texto)\n",
    "\n",
    "data = {'Hora': col_hora,\n",
    "        'Títulos': col_titulos,\n",
    "        'Tipo': col_tipo,\n",
    "        'Contenido':col_contenido}\n",
    "resultado = pd.DataFrame(data)\n",
    "resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.2.2** Captura las noticias sobre economía de Expansion: http://www.expansion.com/economia.html?intcmp=MENUHOM24101&s_kw=economia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Firefox'}\n",
    "url = 'http://www.expansion.com/economia.html?intcmp=MENUHOM24101&s_kw=economia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re = requests.get(url,headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup( re.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_a_extraer = soup.find_all('a',{'class' : 'ue-c-cover-content__link'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for info in datos_a_extraer:\n",
    "    texto = info.text\n",
    "    # print(info.text)\n",
    "    rows.append(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows)\n",
    "df.columns = ['Noticias']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.2.3** Captura la agenda macroeconómica de investing.com: http://es.investing.com/economic-calendar/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Firefox'}\n",
    "url = 'https://es.investing.com/economic-calendar/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re = requests.get(url,headers=headers)\n",
    "soup = BeautifulSoup( re.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_a_extraer = soup.find_all('table',{'class' : 'genTbl closedTbl ecoCalTbl persistArea js-economic-table'})\n",
    "datos_a_extraer = datos_a_extraer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trs = datos_a_extraer.find_all('tr')\n",
    "\n",
    "df = pd.DataFrame(columns=['Hora', 'Evento', 'Actual', 'Previsión', 'Anterior'])\n",
    "\n",
    "for tr in trs:\n",
    "    \n",
    "    tds = tr.find_all('td')\n",
    "    \n",
    "    hora = \"\"\n",
    "    evento = \"\"\n",
    "    actual = \"\"\n",
    "    prevision = \"\"\n",
    "    anterior = \"\"\n",
    "    \n",
    "    for td in tds:\n",
    "        \n",
    "        if \"js-time\" in str(td):\n",
    "            hora = td.text\n",
    "            #print(texto)\n",
    "        \n",
    "        if \"left event\" in str(td):\n",
    "            evento = td.text\n",
    "            evento = evento.replace('\\n',\"\")\n",
    "            #print(texto)\n",
    "        \n",
    "        if \"actual\" in str(td):\n",
    "            actual = td.text\n",
    "            #print(texto)\n",
    "            \n",
    "        if \"forecast\" in str(td):\n",
    "            prevision = td.text\n",
    "            #print(texto)\n",
    "        \n",
    "        if \"previous\" in str(td):\n",
    "            anterior = td.text\n",
    "            #print(texto)\n",
    "    \n",
    "    if hora == \"\":\n",
    "        continue\n",
    "    \n",
    "    df = df.append({'Hora': hora, 'Evento': evento, 'Actual': actual, 'Previsión': prevision, 'Anterior': anterior}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.2.4** Captura las cotizaciones de los commodities de investing.com: https://es.investing.com/commodities/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Firefox'}\n",
    "url = 'https://es.investing.com/commodities/'\n",
    "re = requests.get(url,headers=headers)\n",
    "soup = BeautifulSoup( re.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_a_extraer = soup.find_all('table',{'class' : 'genTbl closedTbl crossRatesTbl'})\n",
    "datos_a_extraer = datos_a_extraer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trs = datos_a_extraer.find_all('tr')\n",
    "\n",
    "df = pd.DataFrame(\"\", index=np.arange(len(trs)), columns=['Material primas', '15 minutos', '1 hora', 'Diario', 'Semanal', 'Mensual', 'Anual', '3 años'])\n",
    "\n",
    "fila = 0\n",
    "\n",
    "for tr in trs:\n",
    "    \n",
    "    tds = tr.find_all('td') \n",
    "    \n",
    "    columna = 0\n",
    "    \n",
    "    for td in tds:\n",
    "        \n",
    "        if \"flag\" in str(td):\n",
    "            continue\n",
    "            \n",
    "        texto = td.text\n",
    "        # print(texto)\n",
    "        df.iloc[fila, columna] = texto\n",
    "        \n",
    "        columna = columna + 1\n",
    "    \n",
    "    fila = fila + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[1:,:]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.2.5** Captura las cotizaciones del Ibex 35 de Bolsa de Madrid: http://www.bolsamadrid.es/esp/aspx/Mercados/Precios.aspx?indice=ESI100000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Firefox'}\n",
    "url = 'http://www.bolsamadrid.es/esp/aspx/Mercados/Precios.aspx?indice=ESI100000000'\n",
    "re = requests.get(url,headers=headers)\n",
    "soup = BeautifulSoup( re.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_a_extraer = soup.find_all('table',{'class' : 'TblPort'})\n",
    "datos_a_extraer = datos_a_extraer[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trs = datos_a_extraer.find_all('tr')\n",
    "\n",
    "df = pd.DataFrame(\"\", index=np.arange(len(trs)), columns=['Nombre', 'Ult', '% Dif', 'Max', 'Min', 'Volumen', 'Efectivo', 'Fecha', 'Hora'])\n",
    "\n",
    "fila = 0\n",
    "\n",
    "for tr in trs:\n",
    "    \n",
    "    tds = tr.find_all('td') \n",
    "    \n",
    "    columna = 0 \n",
    "\n",
    "    for td in tds:\n",
    "        \n",
    "        texto = td.text\n",
    "        # print(texto)\n",
    "        df.iloc[fila, columna] = texto\n",
    "        \n",
    "        columna = columna + 1\n",
    "    \n",
    "    fila = fila + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[1:,:]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.2.6** De la página de bolsa de madrid: https://www.bolsamadrid.es/esp/aspx/Indices/Resumen.aspx obten un daframe con los contenidos de la tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Firefox'}\n",
    "url = 'https://www.bolsamadrid.es/esp/aspx/Indices/Resumen.aspx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "re = requests.get(url,headers=headers)\n",
    "\n",
    "soup = BeautifulSoup( re.text, 'html.parser')\n",
    "table = soup.find_all('table', {'class' : 'TblPort'})\n",
    "table = table[0]\n",
    "\n",
    "trs = table.find_all('tr')\n",
    "ths = table.find_all('th')\n",
    "\n",
    "columns = []\n",
    "for th in ths:\n",
    "    #print(th.string)\n",
    "    columns.append(th.string)\n",
    "\n",
    "rows = []\n",
    "for tr in trs:\n",
    "    #print(tr)\n",
    "    #print(\" \")\n",
    "    \n",
    "    tds = tr.find_all('td')\n",
    "    #print()\n",
    "    #print(\" \")\n",
    "    \n",
    "    data_row = []\n",
    "    for td in tds:\n",
    "        #print(td.string)\n",
    "        #print(\" \")\n",
    "        data_row.append(td.string)\n",
    "    rows.append(data_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows[1:], columns=columns)\n",
    "df = df.set_index('Nombre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.2.7** Obtén las últimas noticias (expansión) y precios objetivo (expansión) de: Telefónica, Santander y Siemens Gamesa\n",
    "\n",
    "Noticias: \n",
    "\n",
    "- https://www.expansion.com/mercados/cotizaciones/valores/telefonica_M.TEF.html\n",
    "- https://www.expansion.com/mercados/cotizaciones/valores/santander_M.SAN.html\n",
    "- https://www.expansion.com/mercados/cotizaciones/valores/siemensgamesa_M.SGRE.html\n",
    "\n",
    "Precios objetivo: \n",
    "\n",
    "- https://www.expansion.com/mercados/bolsa/recomendaciones/consenso-mercados/t/telefonica_M.TEF.html\n",
    "- https://www.expansion.com/mercados/bolsa/recomendaciones/consenso-mercados/s/santander_M.SAN.html\n",
    "- https://www.expansion.com/mercados/bolsa/recomendaciones/consenso-mercados/s/siemensgamesa_M.SGRE.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ultimas_noticias(url):\n",
    "\n",
    "    headers = {'User-Agent': 'Firefox'}\n",
    "    # url = 'https://www.expansion.com/mercados/cotizaciones/valores/telefonica_M.TEF.html'\n",
    "    re = requests.get(url,headers=headers)\n",
    "    soup = BeautifulSoup( re.text, 'html.parser')\n",
    "\n",
    "    datos_a_extraer = soup.find_all('section',{'class' : 'ultimas_noticias_sobre dos-columnas'})\n",
    "    datos_a_extraer = datos_a_extraer[0]\n",
    "    datos_a_extraer = datos_a_extraer.find_all('article',{'class' : 'noticia'})\n",
    "    \n",
    "    # Las noticias siempre son 4, de ahí np.arange(4)\n",
    "    df_noticias = pd.DataFrame(\"\", index=np.arange(4), columns=['Fecha', 'Titulo', 'Entradilla'])\n",
    "\n",
    "    fila = 0\n",
    "\n",
    "    for noticia in datos_a_extraer:\n",
    "\n",
    "        fechas = noticia.find_all('p',{'class' : 'fecha'})    \n",
    "        for fecha in fechas:        \n",
    "            texto = fecha.text\n",
    "            #print(texto)\n",
    "            df_noticias.iloc[fila,0] = texto\n",
    "\n",
    "        titulos = noticia.find_all('h3')    \n",
    "        for titulo in titulos:        \n",
    "            texto = titulo.text\n",
    "            texto = texto.replace('\\n',\"\")\n",
    "            #print(texto)\n",
    "            df_noticias.iloc[fila,1] = texto\n",
    "\n",
    "        entradillas = noticia.find_all('div',{'class' : 'entradilla'})    \n",
    "        for entradilla in entradillas:        \n",
    "            texto = entradilla.text\n",
    "            texto = texto.replace('\\n',\"\")\n",
    "            #print(texto)\n",
    "            df_noticias.iloc[fila,2] = texto\n",
    "\n",
    "        fila = fila + 1       \n",
    "\n",
    "    return df_noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precios_objetivos(url):\n",
    "\n",
    "    headers = {'User-Agent': 'Firefox'}\n",
    "    # url = 'https://www.expansion.com/mercados/bolsa/recomendaciones/consenso-mercados/t/telefonica_M.TEF.html'\n",
    "    re = requests.get(url,headers=headers)\n",
    "    soup = BeautifulSoup( re.text, 'html.parser')\n",
    "\n",
    "    datos_a_extraer = soup.find_all('section',{'class' : 'consenso_mercados'})\n",
    "    datos_a_extraer = datos_a_extraer[0]\n",
    "\n",
    "    datos_a_extraer = datos_a_extraer.find_all('tbody')\n",
    "    datos_a_extraer = datos_a_extraer[0]\n",
    "\n",
    "    trs = datos_a_extraer.find_all('tr')\n",
    "\n",
    "    df_precios_objetivos = pd.DataFrame(\"\", index=np.arange(len(trs)), columns=['Firma', 'Fecha', 'Recomendación', 'Precio objetivo', 'Revaloración potencial'])\n",
    "\n",
    "    fila = 0\n",
    "\n",
    "    for tr in trs:\n",
    "\n",
    "        tds = tr.find_all('td')\n",
    "\n",
    "        columna = 0\n",
    "\n",
    "        for td in tds:\n",
    "\n",
    "            texto = td.text\n",
    "            #print(texto)\n",
    "            df_precios_objetivos.iloc[fila, columna] = texto\n",
    "\n",
    "            columna = columna + 1\n",
    "\n",
    "        fila = fila + 1\n",
    "\n",
    "    return df_precios_objetivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empresas = {\n",
    "    'Telefonica': ['https://www.expansion.com/mercados/cotizaciones/valores/telefonica_M.TEF.html',\n",
    "                  'https://www.expansion.com/mercados/bolsa/recomendaciones/consenso-mercados/t/telefonica_M.TEF.html'], \n",
    "    'Santander': ['https://www.expansion.com/mercados/cotizaciones/valores/santander_M.SAN.html',\n",
    "                 'https://www.expansion.com/mercados/bolsa/recomendaciones/consenso-mercados/s/santander_M.SAN.html'], \n",
    "    'Siemens Gamesa': ['https://www.expansion.com/mercados/cotizaciones/valores/siemensgamesa_M.SGRE.html',\n",
    "                      'https://www.expansion.com/mercados/bolsa/recomendaciones/consenso-mercados/s/siemensgamesa_M.SGRE.html'],\n",
    "}\n",
    "\n",
    "df_ultimas_noticias = ultimas_noticias(empresas['Telefonica'][0])\n",
    "df_precios_objetivos = precios_objetivos(empresas['Telefonica'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ultimas_noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_precios_objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.2.8** Desarrolla una función que sea capaz de capturar todas las noticias que tengan relación con una cualquier empresa y que hayan tenido lugar en un intervalo determinado. Utiliza para ello la hemeroteca de ABC. Pueba a obtener todas la noticias de Telefónica entre el 1 de enero y 31 de enero de 2016.\n",
    "\n",
    "- http://www.abc.es/hemeroteca/resultados-busqueda-avanzada/noticia/pagina-1?exa=telefonica&rfec=20160101;20161031&or=1&nres=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noticias_relacionadas(empresa, fecha_inicio, fecha_fin, pagina = 1):\n",
    "\n",
    "    #empresa = 'Telefonica'\n",
    "    empresa = empresa.lower()\n",
    "    #pagina = 1\n",
    "\n",
    "    #fecha_inicio = '1/1/2016'\n",
    "    #fecha_fin = '31/12/2016'\n",
    "    fechas = pd.to_datetime([fecha_inicio, fecha_fin])\n",
    "    \n",
    "    año_inicio = fechas[0].year\n",
    "    mes_inicio = '{:02d}'.format(fechas[0].month)\n",
    "    dia_inicio = '{:02d}'.format(fechas[0].day)\n",
    "\n",
    "    año_fin = fechas[1].year\n",
    "    mes_fin = '{:02d}'.format(fechas[1].month)\n",
    "    dia_fin = '{:02d}'.format(fechas[1].day)\n",
    "\n",
    "    # url = 'http://www.abc.es/hemeroteca/resultados-busqueda-avanzada/noticia/pagina-1?exa=telefonica&rfec=20160101;20161031&or=1&nres=20'\n",
    "    # Ten cuidado de no tabular la url cortada, o meterás una tabulación en la url y no serás capaz de leer su contenido\n",
    "    url = f'http://www.abc.es/hemeroteca/resultados-busqueda-avanzada/noticia/pagina-{pagina}?exa={empresa}&rfec={año_inicio}\\\n",
    "{mes_inicio}{dia_inicio};{año_fin}{mes_fin}{dia_fin}&or=1&nres=20'\n",
    "    #print(url)\n",
    "\n",
    "    headers = {'User-Agent': 'Firefox'}\n",
    "    re = requests.get(url,headers=headers)\n",
    "    soup = BeautifulSoup( re.text, 'html.parser')\n",
    "\n",
    "    # Lo primero que tenemos que saber es cuantas páginas tendremos que recorrer para obtener todas las noticias\n",
    "    numero_noticias = soup.find('span', {'class': 'total'})\n",
    "    numero_noticias = numero_noticias.text\n",
    "    numero_noticias = numero_noticias.replace('(','').replace(')','')\n",
    "    numero_noticias = int(numero_noticias)\n",
    "    numero_paginas = int(numero_noticias/20)\n",
    "    \n",
    "    noticias = []\n",
    "    \n",
    "    for pagina in range(1,numero_paginas+1):\n",
    "    \n",
    "        # Construimos la url de cada página\n",
    "        url = f'http://www.abc.es/hemeroteca/resultados-busqueda-avanzada/noticia/pagina-{pagina}?exa={empresa}&rfec={año_inicio}\\\n",
    "{mes_inicio}{dia_inicio};{año_fin}{mes_fin}{dia_fin}&or=1&nres=20'\n",
    "        \n",
    "        headers = {'User-Agent': 'Firefox'}\n",
    "        re = requests.get(url,headers=headers)\n",
    "        soup = BeautifulSoup( re.text, 'html.parser')\n",
    "    \n",
    "        datos_a_extraer = soup.find_all('div',{'class' : 'clearfix'})\n",
    "\n",
    "        noticias_pagina = pd.DataFrame(\"\", index=np.arange(20), columns=['Fecha', 'Título', 'Contenido'])\n",
    "\n",
    "        for noticia in datos_a_extraer:\n",
    "\n",
    "            fechas = noticia.find_all('span',{'class' : 'date'})\n",
    "            fila = 0\n",
    "            for fecha in fechas:        \n",
    "                texto = fecha.text\n",
    "                #print(texto)\n",
    "                noticias_pagina.iloc[fila, 0] = texto\n",
    "                fila = fila + 1\n",
    "\n",
    "            titulos = noticia.find_all('h2') \n",
    "            fila = 0\n",
    "            for titulo in titulos:        \n",
    "                texto = titulo.text\n",
    "                texto = texto.replace('\\n',\"\")\n",
    "                #print(texto)\n",
    "                noticias_pagina.iloc[fila, 1] = texto\n",
    "                fila = fila + 1\n",
    "\n",
    "            contenidos = noticia.find_all('p')\n",
    "            fila = 0\n",
    "            for contenido in contenidos:        \n",
    "                texto = contenido.text\n",
    "                texto = texto.replace('\\n',\"\")\n",
    "                texto = texto.replace('\\r\\n',\"\")\n",
    "                #print(texto)\n",
    "                noticias_pagina.iloc[fila, 2] = texto\n",
    "                fila = fila + 1\n",
    "    \n",
    "        noticias.append(noticias_pagina)\n",
    "        resultado = pd.concat(noticias)    \n",
    "    \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empresa = 'Telefonica'\n",
    "fecha_inicio = '1/1/2016'\n",
    "fecha_fin = '31/1/2016'\n",
    "\n",
    "noticias = noticias_relacionadas(empresa = empresa, fecha_inicio = fecha_inicio, fecha_fin = fecha_fin)\n",
    "noticias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  8.3. Web Scraping II.\n",
    "**8.3.1** Cambia en el selector a Ibex Small Cap y obten la tabla de los datos en un dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVER_PATH = 'chromedriver' # El mío está en la carpeta del módulo 8\n",
    "driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "driver.get('https://www.bolsamadrid.es/')\n",
    "\n",
    "boton_acciones = driver.find_element_by_xpath('//*[@id=\"MenuIzq\"]/div[1]/div[4]/div[1]/div/a')\n",
    "boton_acciones.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "botton_small_cap = driver.find_element_by_xpath('//*[@id=\"SelIndice\"]/option[4]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "botton_small_cap.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "botton_small_cap.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "botton_consultar = driver.find_element_by_xpath('//*[@id=\"ctl00_Contenido_Consultar\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "botton_consultar.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla_small_cap = driver.find_element_by_xpath('//*[@id=\"aspnetForm\"]/div[6]') # es el path del div que está encima de table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla_html = tabla_small_cap.get_attribute('innerHTML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_html(tabla_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.3.2** Obtén de Investing.com la información histórica del Banco Santander, desde el 1 de enero del 2018, hasta el 31 de diciembre del mismo año\n",
    "\n",
    "https://es.investing.com/equities/banco-santander-historical-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVER_PATH = 'chromedriver' # El mío está en la carpeta del módulo 8\n",
    "driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "driver.get('https://es.investing.com/equities/banco-santander-historical-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ojo, porque al acceder a investing desde código, lo primero que nos encontraremos serán 2 pop up\n",
    "\n",
    "<center>\n",
    "<img src=\"./entrada_investing.png\"  alt=\"drawing\" width=\"600\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boton_aceptar = driver.find_element_by_xpath('//*[@id=\"onetrust-accept-btn-handler\"]')\n",
    "boton_aceptar.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boton_cerrar = driver.find_element_by_xpath('//*[@id=\"PromoteSignUpPopUp\"]/div[2]/i')\n",
    "boton_cerrar.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boton_acciones = driver.find_element_by_xpath('//*[@id=\"widgetFieldDateRange\"]')\n",
    "boton_acciones.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fecha_inicio = driver.find_element_by_xpath('//*[@id=\"startDate\"]')\n",
    "fecha_inicio.clear()\n",
    "fecha_inicio.send_keys('01/01/2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fecha_fin = driver.find_element_by_xpath('//*[@id=\"endDate\"]')\n",
    "fecha_fin.clear()\n",
    "fecha_fin.send_keys('31/12/2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boton_aceptar = driver.find_element_by_xpath('//*[@id=\"applyBtn\"]')\n",
    "boton_aceptar.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla = driver.find_element_by_xpath('//*[@id=\"results_box\"]')\n",
    "tabla_html = tabla.get_attribute('innerHTML')\n",
    "df = pd.read_html(tabla_html)\n",
    "df[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
